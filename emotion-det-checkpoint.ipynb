{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d772dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "print(\"setup complete , good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f33c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Each folder in the training module...\n",
    "0  -  angry \n",
    "1  -  disgust\n",
    "2  -  fear\n",
    "3  -  happy\n",
    "4  -  neutral\n",
    "5  -  sad\n",
    "6  -  surprise\n",
    "'''\n",
    "train_dir='train/'#Traning dataset--fer2013\n",
    "val_dir = 'test/'\n",
    "Classes=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]#List of classes exact match of your emotions name\n",
    "img_size=224 #image_net 224x224\n",
    "#training_Data=[]\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5b4339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3662 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d80467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52480174",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f792859",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "594082ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d0f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63602241",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a6ec2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9b78fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.0001, decay=1e-6),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e0e1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scipy) (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd032b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "57/57 [==============================] - 27s 471ms/step - loss: 1.2969 - accuracy: 0.5245 - val_loss: 1.5750 - val_accuracy: 0.4195\n",
      "Epoch 2/50\n",
      "57/57 [==============================] - 28s 495ms/step - loss: 1.2775 - accuracy: 0.5361 - val_loss: 1.5567 - val_accuracy: 0.4143\n",
      "Epoch 3/50\n",
      "57/57 [==============================] - 25s 446ms/step - loss: 1.2319 - accuracy: 0.5447 - val_loss: 1.5681 - val_accuracy: 0.4120\n",
      "Epoch 4/50\n",
      "57/57 [==============================] - 26s 452ms/step - loss: 1.2237 - accuracy: 0.5484 - val_loss: 1.6179 - val_accuracy: 0.3941\n",
      "Epoch 5/50\n",
      "57/57 [==============================] - 25s 447ms/step - loss: 1.1966 - accuracy: 0.5692 - val_loss: 1.5485 - val_accuracy: 0.4235\n",
      "Epoch 6/50\n",
      "57/57 [==============================] - 25s 447ms/step - loss: 1.1655 - accuracy: 0.5784 - val_loss: 1.5754 - val_accuracy: 0.4181\n",
      "Epoch 7/50\n",
      "57/57 [==============================] - 25s 448ms/step - loss: 1.1421 - accuracy: 0.5853 - val_loss: 1.5796 - val_accuracy: 0.4110\n",
      "Epoch 8/50\n",
      "57/57 [==============================] - 26s 456ms/step - loss: 1.1087 - accuracy: 0.5962 - val_loss: 1.5627 - val_accuracy: 0.4252\n",
      "Epoch 9/50\n",
      "57/57 [==============================] - 26s 466ms/step - loss: 1.0853 - accuracy: 0.6012 - val_loss: 1.5790 - val_accuracy: 0.4252\n",
      "Epoch 10/50\n",
      "57/57 [==============================] - 26s 462ms/step - loss: 1.0458 - accuracy: 0.6253 - val_loss: 1.5651 - val_accuracy: 0.4360\n",
      "Epoch 11/50\n",
      "57/57 [==============================] - 26s 464ms/step - loss: 1.0241 - accuracy: 0.6253 - val_loss: 1.5965 - val_accuracy: 0.4174\n",
      "Epoch 12/50\n",
      "57/57 [==============================] - 26s 461ms/step - loss: 1.0041 - accuracy: 0.6356 - val_loss: 1.6079 - val_accuracy: 0.4248\n",
      "Epoch 13/50\n",
      "57/57 [==============================] - 26s 461ms/step - loss: 0.9660 - accuracy: 0.6492 - val_loss: 1.5778 - val_accuracy: 0.4316\n",
      "Epoch 14/50\n",
      "57/57 [==============================] - 27s 468ms/step - loss: 0.9437 - accuracy: 0.6506 - val_loss: 1.5660 - val_accuracy: 0.4322\n",
      "Epoch 15/50\n",
      "57/57 [==============================] - 26s 465ms/step - loss: 0.9053 - accuracy: 0.6737 - val_loss: 1.6227 - val_accuracy: 0.4184\n",
      "Epoch 16/50\n",
      "57/57 [==============================] - 26s 465ms/step - loss: 0.8971 - accuracy: 0.6715 - val_loss: 1.6718 - val_accuracy: 0.4057\n",
      "Epoch 17/50\n",
      "57/57 [==============================] - 26s 463ms/step - loss: 0.8617 - accuracy: 0.6901 - val_loss: 1.6442 - val_accuracy: 0.4223\n",
      "Epoch 18/50\n",
      "57/57 [==============================] - 27s 471ms/step - loss: 0.8325 - accuracy: 0.7062 - val_loss: 1.6410 - val_accuracy: 0.4284\n",
      "Epoch 19/50\n",
      "57/57 [==============================] - 28s 490ms/step - loss: 0.7932 - accuracy: 0.7198 - val_loss: 1.6591 - val_accuracy: 0.4362\n",
      "Epoch 20/50\n",
      "57/57 [==============================] - 27s 473ms/step - loss: 0.7611 - accuracy: 0.7374 - val_loss: 1.6775 - val_accuracy: 0.4298\n",
      "Epoch 21/50\n",
      "57/57 [==============================] - 27s 484ms/step - loss: 0.7477 - accuracy: 0.7412 - val_loss: 1.7676 - val_accuracy: 0.4195\n",
      "Epoch 22/50\n",
      "57/57 [==============================] - 27s 473ms/step - loss: 0.7058 - accuracy: 0.7497 - val_loss: 1.6878 - val_accuracy: 0.4332\n",
      "Epoch 23/50\n",
      "57/57 [==============================] - 27s 473ms/step - loss: 0.6982 - accuracy: 0.7563 - val_loss: 1.7062 - val_accuracy: 0.4379\n",
      "Epoch 24/50\n",
      "57/57 [==============================] - 27s 470ms/step - loss: 0.6812 - accuracy: 0.7621 - val_loss: 1.7275 - val_accuracy: 0.4304\n",
      "Epoch 25/50\n",
      "57/57 [==============================] - 27s 473ms/step - loss: 0.6457 - accuracy: 0.7793 - val_loss: 1.7604 - val_accuracy: 0.4216\n",
      "Epoch 26/50\n",
      "57/57 [==============================] - 29s 504ms/step - loss: 0.6166 - accuracy: 0.7840 - val_loss: 1.7304 - val_accuracy: 0.4289\n",
      "Epoch 27/50\n",
      "57/57 [==============================] - 28s 491ms/step - loss: 0.6205 - accuracy: 0.7843 - val_loss: 1.7287 - val_accuracy: 0.4355\n",
      "Epoch 28/50\n",
      "57/57 [==============================] - 27s 475ms/step - loss: 0.5754 - accuracy: 0.8066 - val_loss: 1.7993 - val_accuracy: 0.4414\n",
      "Epoch 29/50\n",
      "57/57 [==============================] - 27s 475ms/step - loss: 0.5666 - accuracy: 0.7982 - val_loss: 1.7871 - val_accuracy: 0.4295\n",
      "Epoch 30/50\n",
      "57/57 [==============================] - 27s 472ms/step - loss: 0.5433 - accuracy: 0.8135 - val_loss: 1.7680 - val_accuracy: 0.4350\n",
      "Epoch 31/50\n",
      "57/57 [==============================] - 28s 489ms/step - loss: 0.5149 - accuracy: 0.8230 - val_loss: 1.8110 - val_accuracy: 0.4268\n",
      "Epoch 32/50\n",
      "57/57 [==============================] - 26s 464ms/step - loss: 0.5012 - accuracy: 0.8266 - val_loss: 1.8565 - val_accuracy: 0.4279\n",
      "Epoch 33/50\n",
      "57/57 [==============================] - 27s 467ms/step - loss: 0.4742 - accuracy: 0.8330 - val_loss: 1.8716 - val_accuracy: 0.4289\n",
      "Epoch 34/50\n",
      "57/57 [==============================] - 26s 464ms/step - loss: 0.4711 - accuracy: 0.8421 - val_loss: 1.8650 - val_accuracy: 0.4223\n",
      "Epoch 35/50\n",
      "57/57 [==============================] - 26s 463ms/step - loss: 0.4378 - accuracy: 0.8469 - val_loss: 1.9505 - val_accuracy: 0.4155\n",
      "Epoch 36/50\n",
      "57/57 [==============================] - 26s 464ms/step - loss: 0.4438 - accuracy: 0.8438 - val_loss: 1.9503 - val_accuracy: 0.4259\n",
      "Epoch 37/50\n",
      "57/57 [==============================] - 27s 475ms/step - loss: 0.4016 - accuracy: 0.8646 - val_loss: 1.9382 - val_accuracy: 0.4322\n",
      "Epoch 38/50\n",
      "57/57 [==============================] - 26s 466ms/step - loss: 0.4006 - accuracy: 0.8627 - val_loss: 1.9192 - val_accuracy: 0.4386\n",
      "Epoch 39/50\n",
      "57/57 [==============================] - 26s 465ms/step - loss: 0.3805 - accuracy: 0.8755 - val_loss: 1.9837 - val_accuracy: 0.4300\n",
      "Epoch 40/50\n",
      "57/57 [==============================] - 26s 464ms/step - loss: 0.3587 - accuracy: 0.8797 - val_loss: 2.0486 - val_accuracy: 0.4196\n",
      "Epoch 41/50\n",
      "57/57 [==============================] - 26s 463ms/step - loss: 0.3624 - accuracy: 0.8769 - val_loss: 2.0292 - val_accuracy: 0.4295\n",
      "Epoch 42/50\n",
      "57/57 [==============================] - 27s 480ms/step - loss: 0.3449 - accuracy: 0.8824 - val_loss: 2.0204 - val_accuracy: 0.4404\n",
      "Epoch 43/50\n",
      "57/57 [==============================] - 26s 465ms/step - loss: 0.3212 - accuracy: 0.8963 - val_loss: 2.0221 - val_accuracy: 0.4322\n",
      "Epoch 44/50\n",
      "57/57 [==============================] - 26s 466ms/step - loss: 0.3267 - accuracy: 0.8880 - val_loss: 2.1140 - val_accuracy: 0.4194\n",
      "Epoch 45/50\n",
      "57/57 [==============================] - 27s 468ms/step - loss: 0.2985 - accuracy: 0.8994 - val_loss: 2.0473 - val_accuracy: 0.4335\n",
      "Epoch 46/50\n",
      "57/57 [==============================] - 29s 505ms/step - loss: 0.2874 - accuracy: 0.9072 - val_loss: 2.0951 - val_accuracy: 0.4275\n",
      "Epoch 47/50\n",
      "57/57 [==============================] - 28s 501ms/step - loss: 0.2842 - accuracy: 0.9033 - val_loss: 2.1340 - val_accuracy: 0.4382\n",
      "Epoch 48/50\n",
      "57/57 [==============================] - 27s 484ms/step - loss: 0.2793 - accuracy: 0.9074 - val_loss: 2.1138 - val_accuracy: 0.4277\n",
      "Epoch 49/50\n",
      "57/57 [==============================] - 28s 489ms/step - loss: 0.2541 - accuracy: 0.9205 - val_loss: 2.2189 - val_accuracy: 0.4272\n",
      "Epoch 50/50\n",
      "57/57 [==============================] - 27s 476ms/step - loss: 0.2608 - accuracy: 0.9141 - val_loss: 2.1814 - val_accuracy: 0.4289\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = emotion_model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch = 3662//64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= 7178// 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee138d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model \n",
    "emotion_model.save('emotion_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c992754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "path=\"haarcascade_frontalface_default.xml\"\n",
    "font_scale=1.5\n",
    "font=cv2.FONT_HERSHEY_PLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d81dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 475\n"
     ]
    }
   ],
   "source": [
    "rectangle_bgr=(255,255,255)\n",
    "\n",
    "img=np.zeros((500,500))\n",
    "\n",
    "text=\"Some texr in a box!\"\n",
    "\n",
    "(text_width,text_height)=cv2.getTextSize(text,font,fontScale=font_scale,thickness=1)[0]\n",
    "\n",
    "text_offset_x=10\n",
    "text_offset_y=img.shape[0]-25\n",
    "print(text_offset_x,text_offset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eceba396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_coords=((text_offset_x,text_offset_y),(text_offset_x+text_width+2,text_offset_y-text_height-2))\n",
    "# print(box_coords)\n",
    "cv2.rectangle(img,box_coords[0],box_coords[1],(0,0,0),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a3aa94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PySimpleGUI\n",
      "  Downloading PySimpleGUI-4.55.1-py3-none-any.whl (394 kB)\n",
      "Installing collected packages: PySimpleGUI\n",
      "Successfully installed PySimpleGUI-4.55.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install PySimpleGUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64ae50b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncap = cv2.VideoCapture(\"vid.mp4\")\\nwhile True:\\n    ret,frame=cap.read()\\n    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades+\\'./haarcascade_frontalface_default.xml\\')\\n    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\\n    faces=faceCascade.detectMultiScale(gray,1.1,4)\\n    status=\"\"\\n    for x,y,w,h in faces:\\n        roi_gray=gray[y:y+h,x:x+w]\\n        roi_color=frame[y:y+h,x:x+w]\\n        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\\n        facess=faceCascade.detectMultiScale(roi_gray)\\n        if len(facess)==0:\\n            print(\"Face not detected\")\\n        else:\\n            for (ex,ey,ew,eh) in facess:\\n                face_roi=roi_color[ey:ey+eh,ex:ex+ew]\\n                final_image=cv2.resize(face_roi,(224,224))\\n                final_image =np.expand_dims(np.expand_dims(cv2.resize(face_roi, (48, 48)), -1), 0)\\n                final_image=np.expand_dims(final_image,axis=0)\\n                final_image=final_image/255.0\\n                font=cv2.FONT_HERSHEY_SIMPLEX\\n                final_image = np.resize(final_image,(1, 48, 48, 1))\\n                Predictions=emotion_model.predict(final_image)\\n                font_scale=1.5\\n                font=cv2.FONT_HERSHEY_PLAIN\\n                status=\"\"\\n                if np.argmax(Predictions)==0:\\n                    status=\"Angry\"\\n                elif np.argmax(Predictions)==1:\\n                    status=\"Disgust\"\\n                elif np.argmax(Predictions)==2:\\n                    status=\"Fear\"\\n                elif np.argmax(Predictions)==3:\\n                    status=\"Happy\"\\n                elif np.argmax(Predictions)==4:\\n                    status=\"Sad\"\\n                elif np.argmax(Predictions)==5:\\n                    status=\"Suprise\"\\n                else:\\n                    status=\"Neutral\"\\n        \\n            x1,y1,w1,h1=0,0,175,75\\n            cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,255,0),-1)\\n            cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255,2))\\n            cv2.putText(frame,status,(100,150),font,3,(0,0,255),2)\\n            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\\n\\n            cv2.imshow(\\'Face emotion Recoginition\\',frame)\\n            if cv2.waitKey(2)&0xFF==ord(\\'q\\'):\\n                break\\n\\n\\ncap.release()\\ncv2.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cap = cv2.VideoCapture(\"vid.mp4\")\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades+'./haarcascade_frontalface_default.xml')\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    faces=faceCascade.detectMultiScale(gray,1.1,4)\n",
    "    status=\"\"\n",
    "    for x,y,w,h in faces:\n",
    "        roi_gray=gray[y:y+h,x:x+w]\n",
    "        roi_color=frame[y:y+h,x:x+w]\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        facess=faceCascade.detectMultiScale(roi_gray)\n",
    "        if len(facess)==0:\n",
    "            print(\"Face not detected\")\n",
    "        else:\n",
    "            for (ex,ey,ew,eh) in facess:\n",
    "                face_roi=roi_color[ey:ey+eh,ex:ex+ew]\n",
    "                final_image=cv2.resize(face_roi,(224,224))\n",
    "                final_image =np.expand_dims(np.expand_dims(cv2.resize(face_roi, (48, 48)), -1), 0)\n",
    "                final_image=np.expand_dims(final_image,axis=0)\n",
    "                final_image=final_image/255.0\n",
    "                font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "                final_image = np.resize(final_image,(1, 48, 48, 1))\n",
    "                Predictions=emotion_model.predict(final_image)\n",
    "                font_scale=1.5\n",
    "                font=cv2.FONT_HERSHEY_PLAIN\n",
    "                status=\"\"\n",
    "                if np.argmax(Predictions)==0:\n",
    "                    status=\"Angry\"\n",
    "                elif np.argmax(Predictions)==1:\n",
    "                    status=\"Disgust\"\n",
    "                elif np.argmax(Predictions)==2:\n",
    "                    status=\"Fear\"\n",
    "                elif np.argmax(Predictions)==3:\n",
    "                    status=\"Happy\"\n",
    "                elif np.argmax(Predictions)==4:\n",
    "                    status=\"Sad\"\n",
    "                elif np.argmax(Predictions)==5:\n",
    "                    status=\"Suprise\"\n",
    "                else:\n",
    "                    status=\"Neutral\"\n",
    "        \n",
    "            x1,y1,w1,h1=0,0,175,75\n",
    "            cv2.rectangle(frame,(x1,x1),(x1+w1,y1+h1),(0,255,0),-1)\n",
    "            cv2.putText(frame,status,(x1+int(w1/10),y1+int(h1/2)),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,0,255,2))\n",
    "            cv2.putText(frame,status,(100,150),font,3,(0,0,255),2)\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255))\n",
    "\n",
    "            cv2.imshow('Face emotion Recoginition',frame)\n",
    "            if cv2.waitKey(2)&0xFF==ord('q'):\n",
    "                break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee0d504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 48, 48, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5880/811641925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#print(img_pixels.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mimg_pixels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_pixels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_pixels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m# find max indexed array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mmax_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\users\\dell\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.preprocessing.image import load_img, img_to_array \n",
    "from keras.models import  load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load model\n",
    "model = load_model(\"emotion_model.h5\")\n",
    "\n",
    "\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + '.\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, test_img = cap.read()  # captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.1, 4)\n",
    "\n",
    "    for (x, y, w, h) in faces_detected:\n",
    "        cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=7)\n",
    "        roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
    "        roi_gray = cv2.resize(roi_gray, (224, 224))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "        img_pixels /= 255\n",
    "        #print(img_pixels.shape)\n",
    "        img_pixels = np.resize(img_pixels,(1,48,48,1))\n",
    "        predictions = model.predict(img_pixels)\n",
    "        # find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index]\n",
    "\n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow('Facial emotion analysis ', resized_img)\n",
    "\n",
    "    if cv2.waitKey(10) == ord('q'):  # wait until 'q' key is pressed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "481030c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = {0:[\"Taming a Powerful Emotion – Gary Chapman\",\"The Anger Trap: Free Yourself from the Frustrations that Sabotage Your Life – Les Carter, Frank Minirth\",\"Letting Go of Anger: The Eleven Most Common Anger Styles And What to Do About Them – Ronald Potter-Efron, Patricia Potter-Efron\",\"Rage: A Step-by-Step Guide to Overcoming Explosive Anger – Ronald Potter-Efron MSW Ph.D.\"],\n",
    "        1:[\"The Art of Happiness, Dalai Lama and Howard Cutler\",\"The Power of Now: A Guide to Spiritual Enlightenment, Eckhart Tolle\",\"The Alchemist, Paulo Coelho\",\"The Charge: Activating the 10 Human Drives That Make You Feel Alive, Brendan Burchard\"],\n",
    "        2:[\"The Big Leap by Gay Hendricks\",\"Daring Greatly by Brené Brown\",\"Rejection Proof by Jia Jiang\",\"Go For No! by Richard Fenton and Andrea Waltz\"],\n",
    "        3:[\"Red, White & Royal Blue by Casey McQuiston\",\"Rough Magic by Lara Prior-Palmer\",\"The Happy Brain by Dean Burnett\",\"Big Magic by Elizabeth Gilbert\"],\n",
    "        5:[\"A Little Life By Hanya Yanagihara\",\"Never Let Me go by Kazuo Ishiguro\",\"Everything i never told you by Celeste Ng\",\"All the light we cannot see by Anthony Doerr\"],\n",
    "        4:[\"Beloved\",\"Minor Feelings: An Asian American Reckoning\",\"When Breath Becomes Air\",\"1984-George Orwell\"],\n",
    "        6:[\"The One-Hundred-Year Old Man Who Climbed Out The Window And Disappeared – Jonas Jonasson\",\"The Adventures Of Sherlock Holmes – Arthur Conan Doyle\",\"An Artist Of The Floating World – Kazuo Ishiguro\",\"We Are All Completely Beside Ourselves – Karen Joy Fowler\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd46054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = {0:[\"Krishnam Vande Jagadgurum\",\"Gamyam\",\"Wake up Sid\",\"3 Idiots\"],\n",
    "         1:[\"Malli Malli Idi Rani Roju\",\"Anand\",\"Rab Ne Bana Di Jodi\",\"Jab We Met\"],\n",
    "         2:[\" It’s Kind of a Funny Story\",\"The Lord of the Rings: Return of the King\",\"Back to the Future\",\"300\"],\n",
    "         3:[\"Pitch Perfect, 2012\",\"Ferris Bueller's Day Off, 1986\",\"Harry Potter and the Philosopher's Stone, 2001\",\"Mary Poppins Returns, 2018\"],\n",
    "         4:[\"SYNECDOCHE, NEW YORK (2008) \",\" THE IRISHMAN (2019) \",\"ANNIHILATION (2018) \",\"MAGNOLIA (1999) \"],\n",
    "         5:[\"Finding Nemo\",\" Love & Mercy\",\"The Sea Inside\",\"Manchester by the Sea\"],\n",
    "         6:[\"Victoria\",\"The Imposter\",\"A Prophet\",\"Eastern Promises\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce8c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "songs = {0:[\"https://gaana.com/song/dhim-thana\",\"https://gaana.com/song/jallantha\",\"https://gaana.com/song/khamoshiyan-2\",\"https://gaana.com/song/bhuladena\"],\n",
    "        1:[\"https://gaana.com/song/ye-chilipi\",\" https://gaana.com/song/cheliya-2\",\"https://gaana.com/song/ishq-wala-love\",\" https://gaana.com/song/dildaara-stand-by-me\"],\n",
    "        2:[\"https://gaana.com/song/inner-demons-29\",\"https://gaana.com/song/its-gonna-be-alright-30\",\"https://gaana.com/song/overcomer-1\",\"https://gaana.com/song/notafraid\"],\n",
    "        3:[\"https://wynk.in/music/song/gangu-leader/sm_A10328E0009126936C\",\"https://gaana.com/song/maahi-vey-1\",\"https://gaana.com/song/why-this-kolaveri-di\"],\n",
    "        4:[\" https://gaana.com/song/lets-go-crazy-12\",\"https://gaana.com/song/good-as-hell-5\",\"https://gaana.com/song/lovely-day-220\",\"https://gaana.com/album/three-little-birds-2018\"],\n",
    "        5:[\" https://gaana.com/song/madhura-nagari\",\"https://gaana.com/song/dosti-from-rrr\",\"https://gaana.com/song/saya-saya-1\",\"https://gaana.com/song/kala-chashma\"],\n",
    "        6:[\"https://gaana.com/playlist/nishasingh-xanxa-dilliwaligirlfriend\",\"https://gaana.com/playlist/dishac-nbbgg-londonthumakda\",\"https://gaana.com/song/shaam-shaandaar-1\",\"https://gaana.com/song/first-class-49\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.preprocessing.image import load_img, img_to_array \n",
    "from keras.models import  load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# load model\n",
    "model = load_model(\"emotion_model1.h5\")\n",
    "\n",
    "# model = load_model(\"best_model.h5\")\n",
    "\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture()\n",
    "\n",
    "sugg = set()\n",
    "\n",
    "prev = 0\n",
    "\n",
    "while True:\n",
    "    ret, test_img = cap.read()  # captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces_detected:\n",
    "        cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=7)\n",
    "        roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
    "        roi_gray = cv2.resize(roi_gray, (224, 224))\n",
    "        img_pixels = image.img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "        img_pixels /= 255\n",
    "        img_pixels = np.resize(img_pixels,(1,48,48,1))\n",
    "#         predictions = model.predict(img_pixels)\n",
    "\n",
    "        # find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion = emotions[max_index]\n",
    "#         predicted_emotion = 'disgust'\n",
    "        \n",
    "        if prev!=max_index:\n",
    "            print(f'for {predicted_emotion} , the suggestions are as follows:')\n",
    "            print('These books might make you feel better :')\n",
    "            for i in books[max_index]:\n",
    "                print(i,'\\n')\n",
    "            print('Listening to these songs might make you feel better :')\n",
    "            for j in songs[max_index]:\n",
    "                print(j,'\\n')\n",
    "            print('Try watching these movies , it might make you feel better :')\n",
    "            for k in movies[max_index]:\n",
    "                print(k,'\\n')\n",
    "        prev=max_index\n",
    "\n",
    "        cv2.putText(test_img, \"angry\", (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow('Facial emotion analysis ', resized_img)\n",
    "\n",
    "    if cv2.waitKey(10) == ord('q'):  # wait until 'q' key is pressed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d113b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
